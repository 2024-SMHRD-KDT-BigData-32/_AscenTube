# main.py
import time
import logging
import os
from datetime import datetime, timezone

from dotenv import load_dotenv

from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from apscheduler.schedulers.background import BackgroundScheduler

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

import mysql.connector
from mysql.connector import Error

from dateutil.parser import isoparse
import isodate 

# --- .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- FastAPI ì•± ë° ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ---
app = FastAPI()
scheduler = BackgroundScheduler(timezone='Asia/Seoul')
is_crawling = False 

# --- CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ ---
origins = [ "http://localhost", "http://localhost:3000", "http://localhost:5173" ]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins, allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

# --- DB ì»¤ë„¥ì…˜ ìƒì„± í•¨ìˆ˜ ---
def get_db_connection():
    try:
        connection = mysql.connector.connect(
            host=os.getenv("DB_HOST"), port=os.getenv("DB_PORT"),
            database=os.getenv("DB_NAME"), user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )
        if connection.is_connected():
            return connection
    except Error as e:
        logger.error(f"DB ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- í¬ë¡¤ë§ ë° DB ì €ì¥ ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” ê³µí†µ í—¬í¼ í•¨ìˆ˜ ---
# main.pyì˜ process_video í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•´ì£¼ì„¸ìš”.

# main.pyì˜ process_video í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•´ì£¼ì„¸ìš”.

# main.pyì˜ process_video í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•´ì£¼ì„¸ìš”.

def process_video(video_key, category_id, youtube_api, cursor, processed_video_keys):
    if video_key in processed_video_keys:
        logger.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ ì˜ìƒì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤ (VIDEO_KEY: {video_key})")
        return

    try:
        # 1. ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        video_detail_request = youtube_api.videos().list(
            part="snippet,contentDetails,statistics",
            id=video_key
        )
        video_detail_response = video_detail_request.execute()
        if not video_detail_response.get("items"): return
        
        video_detail = video_detail_response["items"][0]
        snippet = video_detail["snippet"]
        statistics = video_detail.get("statistics", {})

        # ------------------------------------------------------------------
        # (ìˆ˜ì •) 2. ì±„ë„ ì •ë³´ë¥¼ TB_YT_CHANNELì— ë¨¼ì € ì €ì¥/ì—…ë°ì´íŠ¸
        # ------------------------------------------------------------------
        channel_id = snippet.get("channelId")
        channel_title = snippet.get("channelTitle")
        # (ì‹ ê·œ) ì±„ë„ IDë¥¼ ì´ìš©í•´ í‘œì¤€ ì±„ë„ URL ìƒì„±
        channel_url = f"https://www.youtube.com/channel/{channel_id}"

        # (ìˆ˜ì •) channel_dataì™€ ì¿¼ë¦¬ì— CNL_URL ì¶”ê°€
        channel_data = (channel_id, channel_title, channel_url, channel_title, channel_url)
        channel_insert_query = """
            INSERT INTO TB_YT_CHANNEL (CNL_ID, CNL_NAME, CNL_URL)
            VALUES (%s, %s, %s)
            ON DUPLICATE KEY UPDATE CNL_NAME = VALUES(CNL_NAME), CNL_URL = VALUES(CNL_URL)
        """
        # (ìˆ˜ì •) INSERTì— í•„ìš”í•œ 3ê°œë§Œ ì „ë‹¬í•˜ë„ë¡ ìŠ¬ë¼ì´ì‹±
        cursor.execute(channel_insert_query, channel_data[:3])
        logger.info(f"ì±„ë„ '{channel_title}' ì •ë³´ DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        
        # 3. íƒœê·¸ ì •ë³´ ì¶”ì¶œ ë° ê°€ê³µ
        tags_list = snippet.get("tags", []) 
        video_tags_str = ",".join(tags_list) if tags_list else None

        # (ì´í•˜ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤)
        # 4. TB_VIDEOì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„ ...
        video_data = (
            video_key, channel_id, snippet.get("title"), 
            snippet.get("description"), snippet.get("thumbnails", {}).get("high", {}).get("url"),
            snippet.get("categoryId", category_id), snippet.get("defaultAudioLanguage"),
            'Y',
            isodate.parse_duration(video_detail.get("contentDetails", {}).get("duration", "PT0S")).total_seconds(),
            isoparse(snippet.get("publishedAt")),
            video_tags_str,
            snippet.get("title"), snippet.get("description"), video_tags_str
        )
        
        # 5. TB_VIDEOì— ì €ì¥ ...
        video_insert_query = """
            INSERT INTO TB_VIDEO 
            (VIDEO_KEY, CNL_ID, VIDEO_TITLE, VIDEO_DESC, THUMBNAIL_URL, VIDEO_CATEGORY, 
             VIDEO_LANGUAGE, PUBLIC_YN, VIDEO_PLAYTIME, UPLOADED_AT, VIDEO_TAGS)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
            VIDEO_TITLE = VALUES(VIDEO_TITLE), 
            VIDEO_DESC = VALUES(VIDEO_DESC),
            VIDEO_TAGS = VALUES(VIDEO_TAGS)
        """
        cursor.execute(video_insert_query, video_data[:11])
        logger.info(f"ì˜ìƒ '{snippet['title'][:20]}...' ì •ë³´(íƒœê·¸ í¬í•¨) DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ.")

        # 6. ë°©ê¸ˆ ì €ì¥í•œ ì˜ìƒì˜ PK(VIDEO_ID) ê°€ì ¸ì˜¤ê¸° ...
        cursor.execute("SELECT VIDEO_ID FROM TB_VIDEO WHERE VIDEO_KEY = %s", (video_key,))
        result = cursor.fetchone()
        if not result: return
        db_video_id = result[0]

        # 7. TB_VIDEO_STATSì— í†µê³„ ì •ë³´ ì €ì¥ ...
        stats_data = (
            db_video_id, datetime.now(timezone.utc),
            statistics.get("viewCount"), statistics.get("likeCount"), statistics.get("commentCount")
        )
        stats_insert_query = """
            INSERT INTO TB_VIDEO_STATS 
            (VIDEO_ID, STATS_DATE, VIEW_CNT, LIKE_CNT, CMT_CNT)
            VALUES (%s, %s, %s, %s, %s)
        """
        cursor.execute(stats_insert_query, stats_data)
        logger.info(f"ì˜ìƒ í†µê³„ ì •ë³´ DB ì €ì¥ ì™„ë£Œ.")

        # 8. ëŒ“ê¸€ ìˆ˜ì§‘ ë° TB_COMMENTì— ì €ì¥
        try:
            comment_request = youtube_api.commentThreads().list(part="snippet", videoId=video_key, maxResults=100, order="relevance")
            comment_response = comment_request.execute()
            
            for comment_item in comment_response.get("items", []):
                top_comment = comment_item["snippet"]["topLevelComment"]["snippet"]
                
                # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•©ë‹ˆë‹¤ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
                comment_data = (
                   comment_item["snippet"]["topLevelComment"]["id"], # CMT_ID
                   db_video_id,                                     # VIDEO_ID
                   top_comment.get("authorDisplayName"),            # CMT_WRITE_NAME
                   top_comment.get("textOriginal"),                 # CMT_CONTENT
                   top_comment.get("likeCount", 0),                 # CMT_LIKE_CNT
                   isoparse(top_comment.get("publishedAt")),        # CMT_WRITE_AT
                   datetime.now(timezone.utc),                      # CRAWLED_AT (ì‹ ê·œ ì¶”ê°€)
                   top_comment.get("likeCount", 0)                  # ON DUPLICATE KEY UPDATE ìš©
                )
                comment_insert_query = """
                    INSERT INTO TB_COMMENT 
                    (CMT_ID, VIDEO_ID, CMT_WRITE_NAME, CMT_CONTENT, CMT_LIKE_CNT, CMT_WRITE_AT, CRAWLED_AT)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE CMT_LIKE_CNT = VALUES(CMT_LIKE_CNT)
                """
                # (ìˆ˜ì •) ìŠ¬ë¼ì´ì‹± ì¸ë±ìŠ¤ ë³€ê²½
                cursor.execute(comment_insert_query, comment_data[:-1]) 
                # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

            logger.info(f"ëŒ“ê¸€ {len(comment_response.get('items', []))}ê°œ DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        except HttpError as e:
            if e.resp.status == 403: logger.warn(f"ëŒ“ê¸€ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ëŒ“ê¸€ ë¹„í™œì„±í™” ë“±).")
            else: logger.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

        processed_video_keys.add(video_key)
    except Exception as e:
        logger.error(f"ì˜ìƒ ì²˜ë¦¬(process_video) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
# --------------------------------------------------------------------------
# ìµœì¢… í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜
# --------------------------------------------------------------------------
def run_daily_crawl():
    global is_crawling
    if is_crawling: return

    connection = None
    try:
        is_crawling = True
        logger.info("========== ğŸš€ í†µí•© ì¼ì¼ í¬ë¡¤ë§ ì‹œì‘ ==========")
        
        connection = get_db_connection()
        if not connection: return
        
        api_key = os.getenv("YOUTUBE_API_KEY")
        youtube_api = build("youtube", "v3", developerKey=api_key)

        processed_video_keys = set()

        # --- 1ë‹¨ê³„: ê´€ì‹¬ ì±„ë„ ìµœì‹  ì˜ìƒ ìˆ˜ì§‘ ---
        logger.info("--- 1ë‹¨ê³„: ê´€ì‹¬ ì±„ë„ ìµœì‹  ì˜ìƒ ìˆ˜ì§‘ ì‹œì‘ ---")
        with connection.cursor() as cursor:
            cursor.execute("SELECT DISTINCT CNL_ID FROM TB_FAVORITE_CHANNEL")
            channel_ids = [row[0] for row in cursor.fetchall()]
            logger.info(f"ì´ {len(channel_ids)}ê°œì˜ ê³ ìœ  ê´€ì‹¬ ì±„ë„ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")

            for channel_id in channel_ids:
                logger.info(f"--- ì±„ë„ ID: {channel_id} í¬ë¡¤ë§ ì¤‘ ---")
                request = youtube_api.search().list(part="snippet", channelId=channel_id, order="date", type="video", maxResults=100)
                response = request.execute()
                for item in response.get("items", []):
                    process_video(item["id"]["videoId"], None, youtube_api, cursor, processed_video_keys)
                connection.commit()
                time.sleep(1)

        # --- 2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ë³„ ì¸ê¸° ì˜ìƒ ìˆ˜ì§‘ ---
        logger.info("--- 2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ë³„ ì¸ê¸° ì˜ìƒ ìˆ˜ì§‘ ì‹œì‘ ---")
        with connection.cursor() as cursor:
            category_ids = ["10", "17", "20", "24"]
            for category_id in category_ids:
                logger.info(f"--- ì¹´í…Œê³ ë¦¬ ID: {category_id} í¬ë¡¤ë§ ì¤‘ ---")
                request = youtube_api.videos().list(part="snippet", chart="mostPopular", regionCode="KR", videoCategoryId=category_id, maxResults=100)
                response = request.execute()
                for video in response.get("items", []):
                    process_video(video["id"], category_id, youtube_api, cursor, processed_video_keys)
                connection.commit()
                time.sleep(1)

        logger.info("========== âœ… í†µí•© ì¼ì¼ í¬ë¡¤ë§ ì™„ë£Œ ==========")
        
    except Exception as e:
        logger.error(f"í†µí•© í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if connection: connection.rollback()
    finally:
        is_crawling = False
        if connection and connection.is_connected():
            connection.close()
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --------------------------------------------------------------------------
# API ì—”ë“œí¬ì¸íŠ¸ ë° ìŠ¤ì¼€ì¤„ëŸ¬
# --------------------------------------------------------------------------
@app.post("/api/crawl/trigger-daily", summary="[í†µí•©] ì¼ì¼ ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰")
async def trigger_daily_crawl(background_tasks: BackgroundTasks):
    if is_crawling: raise HTTPException(status_code=409, detail="í¬ë¡¤ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
    background_tasks.add_task(run_daily_crawl)
    return {"message": "í†µí•© ì¼ì¼ í¬ë¡¤ë§ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."}

@app.on_event("startup")
def startup_event():
    scheduler.add_job(run_daily_crawl, 'cron', hour=13, minute=0, id="daily_crawl_job")
    scheduler.start()
    logger.info("APSchedulerê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë§¤ì¼ ì˜¤í›„ 1ì‹œì— í†µí•© í¬ë¡¤ë§ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")

@app.on_event("shutdown")
def shutdown_event():
    scheduler.shutdown()
    logger.info("APSchedulerê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

@app.get("/", summary="ì„œë²„ ìƒíƒœ í™•ì¸")
def read_root():
    return {"status": "Crawling server is running", "crawling_now": is_crawling}